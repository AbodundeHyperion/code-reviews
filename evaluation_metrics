import numpy as np

def confusion_matrix(y_true, y_pred, labels):
matrix = np.zeros((len(labels), len(labels)), dtype=int)
label_to_index = {label: i for i, label in enumerate(labels)}

for true_label, pred_label in zip(y_true, y_pred):
true_index = label_to_index[true_label]
pred_index = label_to_index[pred_label]
matrix[true_index, pred_index] += 1

return matrix

def accuracy(y_true, y_pred):
correct_predictions = sum(t == p for t, p in zip(y_true, y_pred))
return correct_predictions / len(y_true)

def precision(y_true, y_pred, pos_label):
true_positive = sum((t == p == pos_label) for t, p in zip(y_true, y_pred))
predicted_positive = sum(p == pos_label for p in y_pred)
return true_positive / predicted_positive if predicted_positive else 0

def recall(y_true, y_pred, pos_label):
true_positive = sum((t == p == pos_label) for t, p in zip(y_true, y_pred))
actual_positive = sum(t == pos_label for t in y_true)
return true_positive / actual_positive if actual_positive else 0

def f1_score(y_true, y_pred, pos_label):
p = precision(y_true, y_pred, pos_label)
r = recall(y_true, y_pred, pos_label)
return 2 * (p * r) / (p + r) if (p + r) else 0
